---
title: "SwiftPredict plan"
output: html_notebook
---

```{r libs}
library(dplyr)
library(quanteda)
library(data.table)
```

1. corp_dict.R

        + uses files2sentences() to create a corpus split into one-sentence documents from several files
        + tokenizes it into words using str2tokens() that also does some cleaning (remove numbers, weird words etc)
        + creates unigram frequency table that can be used for EDA and to define dictionary
        + creates dict as character vector
        
```{r corp_dict}
toy.corpus <- files2sentences('./data/en_US/buy the book.txt')
toy.tokens <- str2tokens(toy.corpus)
toy.words <- nFreq(1, toy.tokens)
toy.dict <- toy.words$feature
```

2. buildNgrams.R

        + creates a list (1-max_n)-gram frequency tables, using clean set of tokens (takes a while)
        + replaces out-of-dictionary words with <UNK>
        + removeOOD ALSO splits ngrams into words columns
        + recalculates all n-gram frequencies
        + creates data.table instead of data.frames and sets the keys
        
```{r buildNgrams}
toy.ngrams <- lapply(1:4, nFreq, toy.tokens)
toy.ngrams <- lapply(toy.ngrams, removeOOD)
toy.ngrams <- lapply(toy.ngrams, setDT)
lapply(toy.ngrams, function(x) setkeyv(x, colnames(x)[1:length(colnames(x))-1]))
colnames(toy.ngrams[[1]])[1] <- 'X1'
```
        
3. Katz.R

        + my_cond() is a helper function to create logical conditions(X1=.. and X2=.. etc)
        + find_ngram() is also a helper function that tells whether a ngram is observed, and if yes, then its count or percentage
        + estimates probability of a n-gram, given frequency table:
                + for observed ngrams, returns c*/c 
                + for unobserved ngrams, alpha * MLE(tail word)
```{r katz}
```

4. next_word.R
For now, Katz is not used, but stupid backoff is
        
        + stupid_predict returns next word based on any string
        
5. 
        